{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAG12HnielDv"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "! pip install smolagents\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/smolagents.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24vIkmf3elDw"
      },
      "source": [
        "# Text-to-SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4OexeyrelDx"
      },
      "source": [
        "In this tutorial, weâ€™ll see how to implement an agent that leverages SQL using `smolagents`.\n",
        "\n",
        "> Let's start with the golden question: why not keep it simple and use a standard text-to-SQL pipeline?\n",
        "\n",
        "A standard text-to-sql pipeline is brittle, since the generated SQL query can be incorrect. Even worse, the query could be incorrect, but not raise an error, instead giving some incorrect/useless outputs without raising an alarm.\n",
        "\n",
        "ğŸ‘‰ Instead, an agent system is able to critically inspect outputs and decide if the query needs to be changed or not, thus giving it a huge performance boost.\n",
        "\n",
        "Letâ€™s build this agent! ğŸ’ª\n",
        "\n",
        "Run the line below to install required dependencies:\n",
        "```bash\n",
        "!pip install smolagents python-dotenv sqlalchemy --upgrade -q\n",
        "```\n",
        "To call the HF Inference API, you will need a valid token as your environment variable `HF_TOKEN`.\n",
        "We use python-dotenv to load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MXqghQecelDy"
      },
      "outputs": [],
      "source": [
        "#!pip install smolagents python-dotenv sqlalchemy --upgrade -q\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLedYjpQelDy"
      },
      "source": [
        "Then, we setup the SQL environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dgUfUSdrelDy"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        "    Float,\n",
        "    insert,\n",
        "    inspect,\n",
        "    text,\n",
        ")\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\")\n",
        "metadata_obj = MetaData()\n",
        "\n",
        "def insert_rows_into_table(rows, table, engine=engine):\n",
        "    for row in rows:\n",
        "        stmt = insert(table).values(**row)\n",
        "        with engine.begin() as connection:\n",
        "            connection.execute(stmt)\n",
        "\n",
        "table_name = \"receipts\"\n",
        "receipts = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"receipt_id\", Integer, primary_key=True),\n",
        "    Column(\"customer_name\", String(16), primary_key=True),\n",
        "    Column(\"price\", Float),\n",
        "    Column(\"tip\", Float),\n",
        ")\n",
        "metadata_obj.create_all(engine)\n",
        "\n",
        "rows = [\n",
        "    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n",
        "    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n",
        "    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n",
        "    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n",
        "]\n",
        "insert_rows_into_table(rows, receipts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQsD6tSZelDz"
      },
      "source": [
        "### Build our agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-XDMiwhelDz"
      },
      "source": [
        "Now letâ€™s make our SQL table retrievable by a tool.\n",
        "\n",
        "The toolâ€™s description attribute will be embedded in the LLMâ€™s prompt by the agent system: it gives the LLM information about how to use the tool. This is where we want to describe the SQL table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIQtwz3-elDz",
        "outputId": "54d337d5-8f20-4c86-dadd-3b6a672cb22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "  - receipt_id: INTEGER\n",
            "  - customer_name: VARCHAR(16)\n",
            "  - price: FLOAT\n",
            "  - tip: FLOAT\n"
          ]
        }
      ],
      "source": [
        "inspector = inspect(engine)\n",
        "columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n",
        "\n",
        "table_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
        "print(table_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej0_5KV1elD0"
      },
      "source": [
        "```text\n",
        "Columns:\n",
        "  - receipt_id: INTEGER\n",
        "  - customer_name: VARCHAR(16)\n",
        "  - price: FLOAT\n",
        "  - tip: FLOAT\n",
        "```\n",
        "\n",
        "Now letâ€™s build our tool. It needs the following: (read [the tool doc](https://huggingface.co/docs/smolagents/main/en/examples/../tutorials/tools) for more detail)\n",
        "- A docstring with an `Args:` part listing arguments.\n",
        "- Type hints on both inputs and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CeblFgBfelD0"
      },
      "outputs": [],
      "source": [
        "from smolagents import tool\n",
        "\n",
        "@tool\n",
        "def sql_engine(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n",
        "    The table is named 'receipts'. Its description is as follows:\n",
        "        Columns:\n",
        "        - receipt_id: INTEGER\n",
        "        - customer_name: VARCHAR(16)\n",
        "        - price: FLOAT\n",
        "        - tip: FLOAT\n",
        "\n",
        "    Args:\n",
        "        query: The query to perform. This should be correct SQL.\n",
        "    \"\"\"\n",
        "    output = \"\"\n",
        "    with engine.connect() as con:\n",
        "        rows = con.execute(text(query))\n",
        "        for row in rows:\n",
        "            output += \"\\n\" + str(row)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tON4qb8elD0"
      },
      "source": [
        "Now let us create an agent that leverages this tool.\n",
        "\n",
        "We use the `CodeAgent`, which is smolagentsâ€™ main agent class: an agent that writes actions in code and can iterate on previous output according to the ReAct framework.\n",
        "\n",
        "The model is the LLM that powers the agent system. `HfApiModel` allows you to call LLMs using HFâ€™s Inference API, either via Serverless or Dedicated endpoint, but you could also use any proprietary API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "nyZqQrRFelD0",
        "outputId": "ec727a1f-fde8-474f-eb10-6b3bc5bf701d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mCan you give me the name of the client who got the most expensive receipt?\u001b[0m                                      \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m HfApiModel - meta-llama/Meta-Llama-3.1-8B-Instruct \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">Can you give me the name of the client who got the most expensive receipt?</span>                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ HfApiModel - meta-llama/Meta-Llama-3.1-8B-Instruct â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mError in generating model output:\u001b[0m\n",
              "\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: \u001b[0m\u001b[1;33mRoot\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;31m-67d7eece-56fe26ce238f190a52fa6ffb;\u001b[0m\u001b[93m1fe4b7f2-ba83-4d9e-94be-67854361c00f\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\n",
              "\u001b[1;31mBad request:\u001b[0m\n",
              "\u001b[1;31mModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in \u001b[0m\n",
              "\u001b[1;31myour query.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(Request ID: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Root</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-67d7eece-56fe26ce238f190a52fa6ffb;</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">1fe4b7f2-ba83-4d9e-94be-67854361c00f</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Bad request:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">your query.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 0.43 seconds]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.43 seconds]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AgentGenerationError",
          "evalue": "Error in generating model output:\n(Request ID: Root=1-67d7eece-56fe26ce238f190a52fa6ffb;1fe4b7f2-ba83-4d9e-94be-67854361c00f)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0madditional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m             chat_message: ChatMessage = self.model(\n\u001b[0m\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         )\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    969\u001b[0m         )\n\u001b[0;32m--> 970\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: (Request ID: Root=1-67d7eece-56fe26ce238f190a52fa6ffb;1fe4b7f2-ba83-4d9e-94be-67854361c00f)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4b331d8937b4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHfApiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can you give me the name of the client who got the most expensive receipt?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Outputs are returned only at the end. We only look at the last step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     def _run(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mmemory_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_memory_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_start_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_execute_step\u001b[0;34m(self, task, memory_step)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplanning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_first_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {self.step_number}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_answer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer_checks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_final_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmemory_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAgentGenerationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in generating model output:\\n{e}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         self.logger.log_markdown(\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m: Error in generating model output:\n(Request ID: Root=1-67d7eece-56fe26ce238f190a52fa6ffb;1fe4b7f2-ba83-4d9e-94be-67854361c00f)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
          ]
        }
      ],
      "source": [
        "from smolagents import CodeAgent, HfApiModel\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[sql_engine],\n",
        "    model=HfApiModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n",
        ")\n",
        "agent.run(\"Can you give me the name of the client who got the most expensive receipt?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzzBD5TTelD1"
      },
      "source": [
        "### Level 2: Table joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaEAl7_yelD1"
      },
      "source": [
        "Now letâ€™s make it more challenging! We want our agent to handle joins across multiple tables.\n",
        "\n",
        "So letâ€™s make a second table recording the names of waiters for each receipt_id!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSBs6BBNelD1"
      },
      "outputs": [],
      "source": [
        "table_name = \"waiters\"\n",
        "waiters = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"receipt_id\", Integer, primary_key=True),\n",
        "    Column(\"waiter_name\", String(16), primary_key=True),\n",
        ")\n",
        "metadata_obj.create_all(engine)\n",
        "\n",
        "rows = [\n",
        "    {\"receipt_id\": 1, \"waiter_name\": \"Corey Johnson\"},\n",
        "    {\"receipt_id\": 2, \"waiter_name\": \"Michael Watts\"},\n",
        "    {\"receipt_id\": 3, \"waiter_name\": \"Michael Watts\"},\n",
        "    {\"receipt_id\": 4, \"waiter_name\": \"Margaret James\"},\n",
        "]\n",
        "insert_rows_into_table(rows, waiters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHu58KYielD1"
      },
      "source": [
        "Since we changed the table, we update the `SQLExecutorTool` with this tableâ€™s description to let the LLM properly leverage information from this table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtfdC9SVelD1"
      },
      "outputs": [],
      "source": [
        "updated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\n",
        "It can use the following tables:\"\"\"\n",
        "\n",
        "inspector = inspect(engine)\n",
        "for table in [\"receipts\", \"waiters\"]:\n",
        "    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n",
        "\n",
        "    table_description = f\"Table '{table}':\\n\"\n",
        "\n",
        "    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
        "    updated_description += \"\\n\\n\" + table_description\n",
        "\n",
        "print(updated_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06dLxOUaelD1"
      },
      "source": [
        "Since this request is a bit harder than the previous one, weâ€™ll switch the LLM engine to use the more powerful [Qwen/Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVxyZDrmelD1"
      },
      "outputs": [],
      "source": [
        "sql_engine.description = updated_description\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[sql_engine],\n",
        "    model=HfApiModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n",
        ")\n",
        "\n",
        "agent.run(\"Which waiter got more total money from tips?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDw6qBsSelD1"
      },
      "source": [
        "It directly works! The setup was surprisingly simple, wasnâ€™t it?\n",
        "\n",
        "This example is done! We've touched upon these concepts:\n",
        "- Building new tools.\n",
        "- Updating a tool's description.\n",
        "- Switching to a stronger LLM helps agent reasoning.\n",
        "\n",
        "âœ… Now you can go build this text-to-SQL system youâ€™ve always dreamt of! âœ¨"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}