{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAG12HnielDv"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "! pip install smolagents\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/smolagents.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24vIkmf3elDw"
      },
      "source": [
        "# Text-to-SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4OexeyrelDx"
      },
      "source": [
        "In this tutorial, we’ll see how to implement an agent that leverages SQL using `smolagents`.\n",
        "\n",
        "> Let's start with the golden question: why not keep it simple and use a standard text-to-SQL pipeline?\n",
        "\n",
        "A standard text-to-sql pipeline is brittle, since the generated SQL query can be incorrect. Even worse, the query could be incorrect, but not raise an error, instead giving some incorrect/useless outputs without raising an alarm.\n",
        "\n",
        "👉 Instead, an agent system is able to critically inspect outputs and decide if the query needs to be changed or not, thus giving it a huge performance boost.\n",
        "\n",
        "Let’s build this agent! 💪\n",
        "\n",
        "Run the line below to install required dependencies:\n",
        "```bash\n",
        "!pip install smolagents python-dotenv sqlalchemy --upgrade -q\n",
        "```\n",
        "To call the HF Inference API, you will need a valid token as your environment variable `HF_TOKEN`.\n",
        "We use python-dotenv to load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MXqghQecelDy"
      },
      "outputs": [],
      "source": [
        "#!pip install smolagents python-dotenv sqlalchemy --upgrade -q\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLedYjpQelDy"
      },
      "source": [
        "Then, we setup the SQL environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dgUfUSdrelDy"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        "    Float,\n",
        "    insert,\n",
        "    inspect,\n",
        "    text,\n",
        ")\n",
        "\n",
        "engine = create_engine(\"sqlite:///:memory:\")\n",
        "metadata_obj = MetaData()\n",
        "\n",
        "def insert_rows_into_table(rows, table, engine=engine):\n",
        "    for row in rows:\n",
        "        stmt = insert(table).values(**row)\n",
        "        with engine.begin() as connection:\n",
        "            connection.execute(stmt)\n",
        "\n",
        "table_name = \"receipts\"\n",
        "receipts = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"receipt_id\", Integer, primary_key=True),\n",
        "    Column(\"customer_name\", String(16), primary_key=True),\n",
        "    Column(\"price\", Float),\n",
        "    Column(\"tip\", Float),\n",
        ")\n",
        "metadata_obj.create_all(engine)\n",
        "\n",
        "rows = [\n",
        "    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n",
        "    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n",
        "    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n",
        "    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n",
        "]\n",
        "insert_rows_into_table(rows, receipts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQsD6tSZelDz"
      },
      "source": [
        "### Build our agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-XDMiwhelDz"
      },
      "source": [
        "Now let’s make our SQL table retrievable by a tool.\n",
        "\n",
        "The tool’s description attribute will be embedded in the LLM’s prompt by the agent system: it gives the LLM information about how to use the tool. This is where we want to describe the SQL table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIQtwz3-elDz",
        "outputId": "54d337d5-8f20-4c86-dadd-3b6a672cb22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "  - receipt_id: INTEGER\n",
            "  - customer_name: VARCHAR(16)\n",
            "  - price: FLOAT\n",
            "  - tip: FLOAT\n"
          ]
        }
      ],
      "source": [
        "inspector = inspect(engine)\n",
        "columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n",
        "\n",
        "table_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
        "print(table_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej0_5KV1elD0"
      },
      "source": [
        "```text\n",
        "Columns:\n",
        "  - receipt_id: INTEGER\n",
        "  - customer_name: VARCHAR(16)\n",
        "  - price: FLOAT\n",
        "  - tip: FLOAT\n",
        "```\n",
        "\n",
        "Now let’s build our tool. It needs the following: (read [the tool doc](https://huggingface.co/docs/smolagents/main/en/examples/../tutorials/tools) for more detail)\n",
        "- A docstring with an `Args:` part listing arguments.\n",
        "- Type hints on both inputs and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CeblFgBfelD0"
      },
      "outputs": [],
      "source": [
        "from smolagents import tool\n",
        "\n",
        "@tool\n",
        "def sql_engine(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n",
        "    The table is named 'receipts'. Its description is as follows:\n",
        "        Columns:\n",
        "        - receipt_id: INTEGER\n",
        "        - customer_name: VARCHAR(16)\n",
        "        - price: FLOAT\n",
        "        - tip: FLOAT\n",
        "\n",
        "    Args:\n",
        "        query: The query to perform. This should be correct SQL.\n",
        "    \"\"\"\n",
        "    output = \"\"\n",
        "    with engine.connect() as con:\n",
        "        rows = con.execute(text(query))\n",
        "        for row in rows:\n",
        "            output += \"\\n\" + str(row)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tON4qb8elD0"
      },
      "source": [
        "Now let us create an agent that leverages this tool.\n",
        "\n",
        "We use the `CodeAgent`, which is smolagents’ main agent class: an agent that writes actions in code and can iterate on previous output according to the ReAct framework.\n",
        "\n",
        "The model is the LLM that powers the agent system. `HfApiModel` allows you to call LLMs using HF’s Inference API, either via Serverless or Dedicated endpoint, but you could also use any proprietary API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "nyZqQrRFelD0",
        "outputId": "ec727a1f-fde8-474f-eb10-6b3bc5bf701d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mCan you give me the name of the client who got the most expensive receipt?\u001b[0m                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - meta-llama/Meta-Llama-3.1-8B-Instruct \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Can you give me the name of the client who got the most expensive receipt?</span>                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - meta-llama/Meta-Llama-3.1-8B-Instruct ────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mError in generating model output:\u001b[0m\n",
              "\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: \u001b[0m\u001b[1;33mRoot\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;31m-67d7eece-56fe26ce238f190a52fa6ffb;\u001b[0m\u001b[93m1fe4b7f2-ba83-4d9e-94be-67854361c00f\u001b[0m\u001b[1;31m)\u001b[0m\n",
              "\n",
              "\u001b[1;31mBad request:\u001b[0m\n",
              "\u001b[1;31mModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in \u001b[0m\n",
              "\u001b[1;31myour query.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(Request ID: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Root</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-67d7eece-56fe26ce238f190a52fa6ffb;</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">1fe4b7f2-ba83-4d9e-94be-67854361c00f</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
              "\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Bad request:</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">your query.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 0.43 seconds]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.43 seconds]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AgentGenerationError",
          "evalue": "Error in generating model output:\n(Request ID: Root=1-67d7eece-56fe26ce238f190a52fa6ffb;1fe4b7f2-ba83-4d9e-94be-67854361c00f)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0madditional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m             chat_message: ChatMessage = self.model(\n\u001b[0m\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/models.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         )\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcompletion_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    969\u001b[0m         )\n\u001b[0;32m--> 970\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: (Request ID: Root=1-67d7eece-56fe26ce238f190a52fa6ffb;1fe4b7f2-ba83-4d9e-94be-67854361c00f)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4b331d8937b4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHfApiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can you give me the name of the client who got the most expensive receipt?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Outputs are returned only at the end. We only look at the last step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     def _run(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, task, max_steps, images)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mmemory_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_memory_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_start_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAgentGenerationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;31m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36m_execute_step\u001b[0;34m(self, task, memory_step)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplanning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_first_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {self.step_number}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogLevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfinal_answer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer_checks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_final_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/smolagents/agents.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, memory_step)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmemory_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAgentGenerationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in generating model output:\\n{e}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         self.logger.log_markdown(\n",
            "\u001b[0;31mAgentGenerationError\u001b[0m: Error in generating model output:\n(Request ID: Root=1-67d7eece-56fe26ce238f190a52fa6ffb;1fe4b7f2-ba83-4d9e-94be-67854361c00f)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
          ]
        }
      ],
      "source": [
        "from smolagents import CodeAgent, HfApiModel\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[sql_engine],\n",
        "    model=HfApiModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n",
        ")\n",
        "agent.run(\"Can you give me the name of the client who got the most expensive receipt?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzzBD5TTelD1"
      },
      "source": [
        "### Level 2: Table joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaEAl7_yelD1"
      },
      "source": [
        "Now let’s make it more challenging! We want our agent to handle joins across multiple tables.\n",
        "\n",
        "So let’s make a second table recording the names of waiters for each receipt_id!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSBs6BBNelD1"
      },
      "outputs": [],
      "source": [
        "table_name = \"waiters\"\n",
        "waiters = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"receipt_id\", Integer, primary_key=True),\n",
        "    Column(\"waiter_name\", String(16), primary_key=True),\n",
        ")\n",
        "metadata_obj.create_all(engine)\n",
        "\n",
        "rows = [\n",
        "    {\"receipt_id\": 1, \"waiter_name\": \"Corey Johnson\"},\n",
        "    {\"receipt_id\": 2, \"waiter_name\": \"Michael Watts\"},\n",
        "    {\"receipt_id\": 3, \"waiter_name\": \"Michael Watts\"},\n",
        "    {\"receipt_id\": 4, \"waiter_name\": \"Margaret James\"},\n",
        "]\n",
        "insert_rows_into_table(rows, waiters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHu58KYielD1"
      },
      "source": [
        "Since we changed the table, we update the `SQLExecutorTool` with this table’s description to let the LLM properly leverage information from this table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtfdC9SVelD1"
      },
      "outputs": [],
      "source": [
        "updated_description = \"\"\"Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.\n",
        "It can use the following tables:\"\"\"\n",
        "\n",
        "inspector = inspect(engine)\n",
        "for table in [\"receipts\", \"waiters\"]:\n",
        "    columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(table)]\n",
        "\n",
        "    table_description = f\"Table '{table}':\\n\"\n",
        "\n",
        "    table_description += \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
        "    updated_description += \"\\n\\n\" + table_description\n",
        "\n",
        "print(updated_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06dLxOUaelD1"
      },
      "source": [
        "Since this request is a bit harder than the previous one, we’ll switch the LLM engine to use the more powerful [Qwen/Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVxyZDrmelD1"
      },
      "outputs": [],
      "source": [
        "sql_engine.description = updated_description\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[sql_engine],\n",
        "    model=HfApiModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n",
        ")\n",
        "\n",
        "agent.run(\"Which waiter got more total money from tips?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDw6qBsSelD1"
      },
      "source": [
        "It directly works! The setup was surprisingly simple, wasn’t it?\n",
        "\n",
        "This example is done! We've touched upon these concepts:\n",
        "- Building new tools.\n",
        "- Updating a tool's description.\n",
        "- Switching to a stronger LLM helps agent reasoning.\n",
        "\n",
        "✅ Now you can go build this text-to-SQL system you’ve always dreamt of! ✨"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}